{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm import tqdm \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ddi - Mini-Challenge zu LE3, NoSQL\n",
    "Roman Studer, Simon Luder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case\n",
    "<p align=\"center\">\n",
    "  <img src=\"./data/images/Konzept.png\" alt=\"drawing\" width=\"900\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir unterbreiten den Vorschlag eine NoSQL Datenbank (MongoDB) als Data Lake für die Speicherung von Time Series Daten zu verwenden und die Analyse besagter Daten auf einer Time-Series Datenbank durchzuführen. Die Analyse dieser Daten kann mittels einer auf Time Series optimierte Datenbank auf Abruf geschehen. Als Beispiel verwenden wir als Datenquelle openSenseMap, welche Messwerte und Sensormetainformationen über eine API zur Verfügung stellt. Ein aktiver Sensor sendet periodisch (je nach Sensor alle paar Sekunden oder Minuten) einen Messwert. Bei einem Intervall von 10 Sekunden sendet ein Sensor pro Jahr 3'153'600 Datenpunkte. \n",
    "\n",
    "OpenSenseMap erlaubt es ein ganzes Gebiet (Mittels Angabe von Breiten- und Längengrad) zu überwachen. Die Anzahl Sensoren, sowie deren Attribute kann sich über die Zeit ändern. Wenn zum Beispiel ein neuer Sensor im gleichen Gebiet in Betrieb genommen wird. Daher ist die Datenspeicherung in einer NoSQL, schemenlosen Datenbank geeignet. MongoDB ist dabei aufgrund des flexigblem Schema und einfacher horizontaler Skalierung gut geeignet. Dadurch sind wir auf Änderungen in den durch die API erhaltenen Attributen, sowie auf grosse Änderungen in der Datenmenge gewappnet. MongoDB ist allerdins nicht für die Analyse von Time Series geeignet. InfluxDB, eine Time Series Datenbank, ist für Time Series Daten optimiert und kann schnell Aggregationen über eine grosse Anzahl von Datenpunkten (über Timestamp Indexiert) durchführen. Ein Beispiel wäre ein Moving Average mit kleinem Fenster über mehrere Millionen Datenpunkte.\n",
    "\n",
    "Weiter existiert ein MongoDB-Plugin auf Telegraf welche die Performance der MongoDB überwachen kann. Somit kann das Monitoring über Influx betrieben werden. \n",
    "\n",
    "Im Anschluss setzen wir sowohl eine MongoDB als auch eine InfluxDB auf. Über ein Script laden wir alle Sensordaten auf dem Gelände der ETH-Zürich, welche OpenSenseMap zur Verfügung stellt herunter und speichern diese in der MongoDB. Im Anschluss messen wir die Zeit für Aggregationen bei steigenden Datenpunkten einzeln für beide Datenbanken, sowie für den Fall wenn MongoDB als Datalake verwendet wird. Dadurch können wir einen Punkt identifizieren ab dem es nicht mehr sinnvoll ist nur mit einer MongoDB zu arbeiten, sondern die InfluxDB zur Analyse hinzuzuziehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergleich MongoDB vs. InfluxDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenmodell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of ifu boxes at eth\n",
    "url = 'https://api.opensensemap.org/boxes?'\n",
    "bbox = '8.50269672304309, 47.40598032642525,  8.512126181507432, 47.4113301084323 ' # boundary box around eth zurich\n",
    "boxes = requests.get(url, params={'bbox':bbox, 'full':'false'}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date = ''\n",
    "data_format = 'csv'\n",
    "\n",
    "for box in tqdm(boxes):\n",
    "    box_id = box['_id']\n",
    "    box_name = box['name']\n",
    "    location = box['currentLocation']['coordinates']\n",
    "    lat, lon = location[0], location[1]\n",
    "    for sensor in box['sensors']:\n",
    "        try:\n",
    "            sensor_id = sensor['_id']\n",
    "            sensor_name = sensor['title']\n",
    "            sensor_unit = sensor['unit']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        url = f'https://api.opensensemap.org/boxes/{box_id}/data/{sensor_id}?format={data_format}&download=true'\n",
    "        r = requests.get(url, stream=True)\n",
    "        if (len(r.text) > 16): # check if sensor returns values (header has length 16)\n",
    "            with open(f'./data/{box_name}_{sensor_name}.csv', 'wb') as f:\n",
    "                for _, line in enumerate(r.iter_lines()):\n",
    "                    if _ == 0: # define header\n",
    "                        line = 'box_name,sensor_name,box_id,sensor_id,lat,lon,unit,current_time,value\\n'\n",
    "                    else:\n",
    "                        time, value = (line.decode(\"utf-8\").split(','))\n",
    "                        time = time.replace('T', ' ').replace('Z', '')\n",
    "                        time = datetime.strptime(time, '%Y-%m-%d %H:%M:%S.%f').strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        line= f'{box_name},{sensor_name},{box_id},{sensor_id},{lat},{lon},{sensor_unit},{time},{value}\\n'\n",
    "                    f.write(line.encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymongo\n",
    "from pymongo import MongoClient\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_dict(path, file):\n",
    "    '''converts a csv file to a dictionary'''\n",
    "    data = pd.read_csv(path + file)\n",
    "    pd.to_datetime(data.current_time, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    dictionary = dict()\n",
    "    dictionary[\"_id\"] = file.replace('.csv', '')\n",
    "    dictionary[\"box_name\"] = data[\"box_name\"][0]\n",
    "    dictionary[\"sensor_name\"] = data[\"sensor_name\"][0]\n",
    "    dictionary[\"box_id\"] = data[\"box_id\"][0]\n",
    "    dictionary[\"sensor_id\"] = data[\"sensor_id\"][0]\n",
    "    dictionary[\"lat\"] = data[\"lat\"][0]\n",
    "    dictionary[\"lon\"] = data[\"lon\"][0]\n",
    "    dictionary[\"unit\"] = data[\"unit\"][0]\n",
    "    dictionary[\"measurments\"] = dict(zip(data[\"current_time\"], data[\"value\"]))\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = pymongo.MongoClient('localhost', 27017)\n",
    "mongo_db = mongo_client[\"ddi_mc2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "path = './data/'\n",
    "for file in tqdm(os.listdir(\"./data\")):\n",
    "#     print(file.split(\"_\")[0])\n",
    "    if file.endswith('.csv'): # check for filetype\n",
    "        if not mongo_db[file.split(\"_\")[0]].count_documents({\"_id\":file.replace('.csv', '')}) > 0:\n",
    "            dictionary = csv_to_dict(path, file)\n",
    "            mongo_db[file.split(\"_\")[0]].insert_one(dictionary)\n",
    "            if VERBOSE:\n",
    "                print(\"populate:\", file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_db[\"IfU SenseBox2021 11A\"].find().distinct('_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup InfluxDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install influxdb-client\n",
    "from datetime import datetime\n",
    "\n",
    "from influxdb_client import InfluxDBClient, Point, WritePrecision\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "\n",
    "# You can generate a Token from the \"Tokens Tab\" in the UI\n",
    "token = \"AW-zLqzOTpQW4sRYaKbdXpSxBLkxT8rT-RZA-IS5MYo41RZ40YoOCoNYTyu9S2La5W4KpcDzDgCfj53fk6aZuw==\"\n",
    "org = \"ddi\"\n",
    "bucket = \"ddi\"\n",
    "\n",
    "client = InfluxDBClient(url=\"http://localhost:8086\", token=token)\n",
    "write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "\n",
    "# RS: uZei_UmVg7IGcllVQdvKbmbCjwx5s0pe7KfTafVspsL0qGWIg6fmB34JNwWmsEGdt9aFr2Qio6ltOB9_ZrCDDw==\n",
    "# SL: AW-zLqzOTpQW4sRYaKbdXpSxBLkxT8rT-RZA-IS5MYo41RZ40YoOCoNYTyu9S2La5W4KpcDzDgCfj53fk6aZuw=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.health()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query MongoDB, insert into InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select box and get all measurements\n",
    "col = mongo_db.list_collection_names()[0]\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_names = mongo_db[col].find().distinct('_id')\n",
    "id_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = mongo_db[col].find_one({},{'measurments':1}) # select first sensor with field 'measurments'\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = mongo_db[col].find_one({'_id':id_names[0]},{'measurments'})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for observation in tqdm(result['measurments'].items()):\n",
    "    point = Point(result['_id'].split('_')[0]) \\\n",
    "      .tag(\"sensor_name\", result['_id'].split('_')[1]) \\\n",
    "      .field(\"_value\", observation[1])\\\n",
    "      .time(datetime.strptime(observation[0], \"%Y-%m-%d %H:%M:%S\"), WritePrecision.S)\n",
    "\n",
    "    write_api.write(bucket, org, point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''from(bucket: \"{bucket}\") \n",
    "        |> range(start: -2d)'''\n",
    "tables = client.query_api().query(query, org=org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
