{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import warnings\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "from datetime import datetime\n",
    "from isodate import datetime_isoformat\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ddi - Mini-Challenge zu LE3, NoSQL\n",
    "Roman Studer, Simon Luder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case\n",
    "<p align=\"center\">\n",
    "  <img src=\"./data/images/Konzept.png\" alt=\"drawing\" width=\"900\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir unterbreiten den Vorschlag eine NoSQL Datenbank (MongoDB) als Data Lake für die Speicherung von Time Series Daten zu verwenden und die Analyse besagter Daten auf einer Time-Series Datenbank durchzuführen. Die Analyse dieser Daten kann mittels einer auf Time Series optimierte Datenbank auf Abruf geschehen. Als Beispiel verwenden wir als Datenquelle openSenseMap, welche Messwerte und Sensormetainformationen über eine API zur Verfügung stellt. Ein aktiver Sensor sendet periodisch (je nach Sensor alle paar Sekunden oder Minuten) einen Messwert. Bei einem Intervall von 10 Sekunden sendet ein Sensor pro Jahr 3'153'600 Datenpunkte. \n",
    "\n",
    "OpenSenseMap erlaubt es ein ganzes Gebiet (Mittels Angabe von Breiten- und Längengrad) zu überwachen. Die Anzahl Sensoren, sowie deren Attribute kann sich über die Zeit ändern. Wenn zum Beispiel ein neuer Sensor im gleichen Gebiet in Betrieb genommen wird. Daher ist die Datenspeicherung in einer NoSQL, schemenlosen Datenbank geeignet. MongoDB ist dabei aufgrund des flexigblem Schema und einfacher horizontaler Skalierung gut geeignet. Dadurch sind wir auf Änderungen in den durch die API erhaltenen Attributen, sowie auf grosse Änderungen in der Datenmenge gewappnet. MongoDB ist allerdins nicht für die Analyse von Time Series geeignet. InfluxDB, eine Time Series Datenbank, ist für Time Series Daten optimiert und kann schnell Aggregationen über eine grosse Anzahl von Datenpunkten (über Timestamp Indexiert) durchführen. Ein Beispiel wäre ein Moving Average mit kleinem Fenster über mehrere Millionen Datenpunkte.\n",
    "\n",
    "Weiter existiert ein MongoDB-Plugin auf Telegraf welche die Performance der MongoDB überwachen kann. Somit kann das Monitoring über Influx betrieben werden. \n",
    "\n",
    "Im Anschluss setzen wir sowohl eine MongoDB als auch eine InfluxDB auf. Über ein Script laden wir alle Sensordaten auf dem Gelände der ETH-Zürich, welche OpenSenseMap zur Verfügung stellt herunter und speichern diese in der MongoDB. Im Anschluss messen wir die Zeit für Aggregationen bei steigenden Datenpunkten einzeln für beide Datenbanken. Damit können wir Unterschiede in der Performance von Influx und Mongo aufzeigen. Zum Schluss zeigen wir wie ein Punkt identifizieren werden kann ab dem es nicht mehr sinnvoll ist nur mit einer MongoDB zu arbeiten, sondern die InfluxDB zur Analyse hinzuzuziehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergleich MongoDB vs. InfluxDB\n",
    "\n",
    "Einen Einblick in die beiden Datenbanken zu erhalten, erlaubt dass Dokument [1] \"Benchmarking InfluxDB vs. MonoDB for Time Series Data, Metrics % Management\" (siehe Anhang),\n",
    "\n",
    "| Bereich  | InfluxDB  |MongoDB  | \n",
    "|---|---|---|\n",
    "| **Verwendung** | Time-Series Datenbank | Dokumentenbasierte Datenbank, NoSQL Datenbank | \n",
    "| **Memory-Auslastung**  | ca 2-4 GB pro <100'000 Einträge [2]  | ca. 1 GB pro 100'000 Einträge [3] |\n",
    "| **Festplattenauslastung über 24h** [1]  | 178 MB |  34820 MB  |\n",
    "| **Abfragegeschwindigkeit** (Queries per Second, 1000 Einträge) [1] |  935 | 164  |\n",
    "| **Einfügegeschwindigkeit** (Werte pro Sekunde) [1] | 2,800,990  | 1,114,616  |\n",
    "| **Skalierbarkeit**  | Horizontal Skalierbar (Clustering) bei InfluxDB Enterprise [4] | Horizontal Skalierbar  |\n",
    "| **Sicherheit**  | Authentifizierung  | Authentifizierung/Verschlüsselung  |\n",
    "| **Sprache**[1]  | C/C++  | Go |\n",
    "| **Schema**[1]  | Schemaless   | Schemaless |\n",
    "\n",
    "**Verwendung:** MongoDB ist speziell für Internet- oder Business-Applikationen geeignet welche skalierbar und agil sein müssen. Da MongoDB eine Dokumentenbasierte Datenbank ist, ist sie Schemalos und kann sich somit gut an einem sich wandelnden Environment anpassen [7]. Die Influxdatenbank hingegen ist eine auf Time-Series spezialisierte Datenbank die einen Timestamp als Index verwendet. Sie ist somit für Applikationen oder System geeignet die eine hohe Anzahl von Senordaten, bzw. Log-Daten erhählt und kann schnell Aggregationen wie moving Averages erstellen. \n",
    "\n",
    "**Skalierbarkeit:** Die Daten in einer MongoDB sind nicht durch Relationen untereinander verbunden, sondern werden als Dokumente in einem JSON-Ähnlichen Format abgelegt. Diese Dokmente sind voneinander unabhängig, was die Verteilung über mehrere Nodes einfach ermöglicht, und somit auch das horizontale Skalieren. Vertikales Skalieren, also verfügbare Leistung der CPU oder RAM eines einzelnen Servers zu erhöhen ist ebenfalls möglich [8]. Die Enterprise Edition der InfluxDB erlaubt Clustering, wobei Data Nodes sowie Meta Nodes erstellt werden müssen. Die Meta Nodes beschreiben die Cluster, behalten im gewissen Sinne den Überblick, währenddem die Data Nodes die Daten festhalten, sowie Queries Clusterübergreigend über ein *'Protobuf'*-Protokoll druchführen [4]\n",
    "\n",
    "**Sicherheit:** MongoDB bietet diverse Features die die Sicherheit der Datenbank erhöhen können. Die Datenbank unterstützt under anderem mehrere Authentifizierungsmechanismen wie `SCRAM` (\"Salted Challenge Response Authentication Mechanism\", mit SHA-1 oder SHA-256), womit Name, Passwort, Authentifizierungsdatenbank geprüft werden, aber auch das `x-509 Certificate`, die `LDAP`- und `Kerberos`-Authentifizierung. Zugriff kann durch eine rollenbasierte Zugriffskontrolle reguliert werden. So können Benutzer erstellt werden, welchen ähnlich zu einem Active Directory Rollen (wie zum Beispiel \"Administrator\") vergeben werden können, welche ihren Handlungsbereich einschränkt. Ein weiteres Feature ist die Client-Side Field Level Encryption. So können Felder eines Dokumentes vor der Übermittlung zum Server bereits verschlüsselt werden.(vgl. [5]) Die InfluxDB bietet ebenfalls Authentifizierungsmethoden an (darunter Authentifizierung über CLI, JWT Token oder über die API). Dabei wird ebenfalls ein Rollensystem verwendet welches die Verwaltung von Benutzern und derer Berechtigungen ermöglicht. Die Kommunikation zwischen Client und Server läuft über HTTPS, wobei die Authentizität des Servers geprüft wird. (vgl. [6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenmodell\n",
    "\n",
    "Die verwendeten Daten sind Konzeptionell sehr einfach strukturiert. Die verwendeten Daten sind übergeordnet einer Messbox zugeordnet. Bei der Messbox sind Informationen wie der Name und die ID, aber auch der Standort anhand von Längen-, und Breitengraden angegeben. Innerhalb einer Messbox sind ein oder mehrere unterschiedliche Sensoren vorhanden.  Ein einzelner Sensor hat wieder eine eigene Bezeichnung, Sensor-ID, misst eine bestimmte Messgrösse und erstellt wiederum in regelmässigen Abständen Einträge, welche Informationen zum Messwert und dem Messzeitpunkt enthalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./data/images/ER-Diagram.png\" alt=\"drawing\" width=\"450\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of ifu boxes at eth\n",
    "url = 'https://api.opensensemap.org/boxes?'\n",
    "bbox = '8.50269672304309, 47.40598032642525,  8.512126181507432, 47.4113301084323 ' # boundary box around eth zurich\n",
    "boxes = requests.get(url, params={'bbox':bbox, 'full':'false'}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date = '2021-05-12T10:05:49.581Z'\n",
    "to_date = '2021-05-20T10:05:49.581Z'\n",
    "data_format = 'csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = pd.date_range(from_date,to_date,freq='4h')\n",
    "\n",
    "def to_RFC3339Date(x):\n",
    "    x = str(x)\n",
    "    x = x.replace(' ','T')\n",
    "    x = x.replace('000+00:00','Z')\n",
    "    return x\n",
    "\n",
    "times = [i for i in times][1:] # remove first time as it is equal to from_date\n",
    "times = [to_RFC3339Date(i) for i in times]\n",
    "len(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for box in tqdm(boxes):\n",
    "    box_id = box['_id']\n",
    "    box_name = box['name']\n",
    "    location = box['currentLocation']['coordinates']\n",
    "    lat, lon = location[0], location[1]\n",
    "    for sensor in box['sensors']:\n",
    "        try:\n",
    "            sensor_id = sensor['_id']\n",
    "            sensor_name = sensor['title']\n",
    "            sensor_name.replace('/', '')\n",
    "            sensor_unit = sensor['unit']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #url = f'https://api.opensensemap.org/boxes/{box_id}/data/{sensor_id}?format={data_format}&download=true'\n",
    "        url = f'https://api.opensensemap.org/boxes/{box_id}/data/{sensor_id}?from-date={from_date}&to-date={to_date}&download=true&format={data_format}'\n",
    "        r = requests.get(url, stream=True)\n",
    "        if (len(r.text) > 16): # check if sensor returns values (header has length 16)\n",
    "            with open(f'./data/{box_name}_{sensor_name}.csv', 'wb') as f:\n",
    "                for _, line in enumerate(r.iter_lines()):\n",
    "                    if _ == 0: # define header\n",
    "                        line = 'box_name,sensor_name,box_id,sensor_id,lat,lon,unit,current_time,value\\n'\n",
    "                    else:\n",
    "                        timest, value = (line.decode(\"utf-8\").split(','))\n",
    "                        timest = timest.replace('T', ' ').replace('Z', '')\n",
    "                        timest = datetime.strptime(timest, '%Y-%m-%d %H:%M:%S.%f').strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        line= f'{box_name},{sensor_name},{box_id},{sensor_id},{lat},{lon},{sensor_unit},{timest},{value}\\n'\n",
    "                    f.write(line.encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Abschnitt befasst sich mit dem Aufsetzen und Abfragen der Sensordaten in die MongoDB. Wenn man eine MongoDB zum Abspeichern von Zeitreihen-Daten verwenden möchte, dann gibt es mehrere Ansätze in der Struktur, die Daten organisiert werden kann. \n",
    "\n",
    "Wir verwenden für die MongoDB die \"Single-Document\" Methode werden alle Messpunkte einzeln Abgespeichert. Für jede Messung eines Sensors (oder jedes Intervall) wird also ein  eigenes Dokument in der Datenbank erstellt.\n",
    "Das Dokument enthält Daten wie den Messwert, Messzeitpunkt aber auch die anderen Informationen des Sensors und der Messbox. Der Nachteil dieser Methode ist, dass grundlegende Informationen über den Sensor oder die Messbox, welche Konstant bleiben sich in jedem Dokument wiederholen, wodurch die absolute Grösse unserer Collection deutlich anwächst und die Performance der Datenbank beeinträchtigen kann. Dafür bleiben einmal erstellte Dokumente in der Datenbank unberührt und sind somit weniger anfällig für Fehler in der Datenbeschaffung.\n",
    "\n",
    "Alternativ bestünde die Möglichkeit die Daten in einer \"Time-Based bucketing\" Methode in zu strukturieren. Dabei könnten zum Beispiel im Minutentakt die Messwerte aller Sensoren in einem Dokument zusammengefasst werden. Dadurch würde auch die Grösse unserer Collection kleiner. Eine Voraussetzung für diese Methode ist allerdings, dass die Sensoren im gleichen Takt oder Zeitintervall Messwerte liefern, was bei unseren Daten nicht gegeben ist. (vgl. [9])\n",
    "\n",
    "Aus diesem Grund haben wir uns für die erste Methode entschieden, beider für jeden Messpunkt ein einzelnes Dokument erstellt wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "Die Daten sind ursprünglich als \".csv\" Format vorhanden und müssen deshalb zuerst noch in eine JSON ähnliche Struktur umgewandelt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_dict(path, file):\n",
    "    '''converts a csv file to a list of dictionaries'''\n",
    "    data = pd.read_csv(path + file)\n",
    "    pd.to_datetime(data.current_time, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    document = []\n",
    "    for i in range(len(data)):\n",
    "        dictionary = dict()\n",
    "        dictionary[\"_id\"] = file.replace('.csv', '_{}'.format(i))\n",
    "        dictionary[\"box_name\"] = data[\"box_name\"][0]\n",
    "        dictionary[\"sensor_name\"] = data[\"sensor_name\"][0]\n",
    "        dictionary[\"box_id\"] = data[\"box_id\"][0]\n",
    "        dictionary[\"sensor_id\"] = data[\"sensor_id\"][0]\n",
    "        dictionary[\"lat\"] = data[\"lat\"][0]\n",
    "        dictionary[\"lon\"] = data[\"lon\"][0]\n",
    "        dictionary[\"unit\"] = data[\"unit\"][0]\n",
    "        dictionary[\"value\"] = data[\"value\"][i]\n",
    "        dictionary[\"timestamp\"] = dt.datetime.strptime(data[\"current_time\"][i], '%Y-%m-%d %H:%M:%S')\n",
    "        document.append(dictionary)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create db\n",
    "\n",
    "Der folgende Code wird verwendet, um unsere Datenbank mit dem Namen ddi_mc2 zu initialisieren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "import datetime as dt\n",
    "\n",
    "mongo_client = pymongo.MongoClient('localhost', 27017)\n",
    "mongo_db = mongo_client[\"ddi_mc2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate db\n",
    "\n",
    "In der nächsten Zelle werden die Sensoren in die MongoDB geladen. Als erster Schritt werden alle Dokumente mit der Endung \".csv\" gesucht. Ein einzelnes Dokument enthält jeweils alle Informationen inklusive der Messwerte für einen einzelnen Sensor. Danach wird jedes Dokument, mit der funktion `csv_to_dict()`, pro Zeile getrennt in ein Dictionary umgewandelt. Die Dictionaries werden dann einzeln mit dem Befehl `insert_one()` in die MongoDB geladen. \n",
    "\n",
    "Die Messwerte der einzelnen Sensoren sind in der MongoDB jeweils nach ihrer Zugehörigkeit zu einer Messbox, in einer Collection zusammengefasst. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "path = './data/'\n",
    "for file in tqdm(os.listdir(\"./data\")):\n",
    "#     print(file.split(\"_\")[0])\n",
    "    if file.endswith('.csv'): # check for filetype\n",
    "        if not mongo_db[file.split(\"_\")[0]].count_documents({\"_id\":file.replace('.csv', '')}) > 0:\n",
    "            dictionary = csv_to_dict(path, file)\n",
    "            for document in dictionary:\n",
    "                mongo_db[file.split(\"_\")[0]].insert_one(document)\n",
    "            if VERBOSE:\n",
    "                print(\"populate:\", file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Sensor-Boxes within the MongoDB\n",
    "Mit dem Befehl list_collection_names() werden alle in der Datenbank enthaltenen Collections angezeigt. In unserem fall, entspricht das der Anzahl an Messboxen in der Datenbank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_db.list_collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Sensors within a Collection\n",
    "Nachfolgend sind alle Sensoren aufgelistet, welche in unserer Collection für die Messbox `IFU Sensebox2021 2A` enthalten sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = mongo_db['IFU Sensebox2021 2A'].find().distinct('sensor_name')[0:10]\n",
    "boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query MongoDB\n",
    "\n",
    "Wir fragen nun die Daten für den Sensor `Lufttemperatur 2-BME680` der Messbox `IFU Sensebox2021 2A` für unterschiedliche Zeitintervalle ab. Die Zeitintervalle beginnen bei 4 Stunden und werden Pro Iteration um 4 Stunden erhöht. Es wird deshalb erwartet, das sich die Abfragedauer pro Iteration jeweils leicht erhöht. Instesammt werden 48 unterscheidliche Zeitintervalle abgefragt, wobei die längste Zeitspanne 8 Tage beträgt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_name = \"Lufttemperatur 2-BME680\"\n",
    "box_name = \"IFU Sensebox2021 2A\"\n",
    "\n",
    "mean_runtimes = []\n",
    "std_runtimes = []\n",
    "n_init = 5\n",
    "\n",
    "for i in tqdm(range(len(times))):   \n",
    "    runtimes = [] \n",
    "    n_datepoints = []\n",
    "    \n",
    "    for _ in range(n_init):\n",
    "\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        query = list(mongo_db[box_name].aggregate([\n",
    "                {\n",
    "                    \"$match\": {\n",
    "                        \"sensor_name\": sensor_name,\n",
    "                        \"timestamp\": {\"$gte\": datetime.strptime(from_date, '%Y-%m-%dT%H:%M:%S.%fZ'), \n",
    "                                      \"$lte\": datetime.strptime(times[i], '%Y-%m-%dT%H:%M:%S.%fZ')}\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"$group\": {\n",
    "                        \"_id\": {\n",
    "                        \"minute\": {\"$minute\": \"$timestamp\"},\n",
    "                        \"hour\": {\"$hour\": \"$timestamp\"},\n",
    "                        \"day\": {\"$dayOfMonth\": \"$timestamp\"},\n",
    "                        \"month\": {\"$month\": \"$timestamp\"},\n",
    "                        \"year\": {\"$year\": \"$timestamp\"} \n",
    "                        }, \n",
    "                        \"mean\": {\"$avg\": \"$value\"},\n",
    "                        \"unit\":{\"$first\": \"$unit\"},\n",
    "                        \"timestamp\": { \"$max\": \"$timestamp\" }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"$project\":{\n",
    "                        \"_id\":0,\n",
    "                        \"mean\": \"$mean\",\n",
    "                        \"unit\": \"$unit\",\n",
    "                        \"timestamp_minute\": {\"$dateToString\":{\"format\": \"%Y-%m-%d %H:%M:00\", \n",
    "                                                              \"date\": \"$timestamp\" }}\n",
    "                    }\n",
    "                },\n",
    "                { \n",
    "                    \"$sort\": {\n",
    "                        \"timestamp_minute\" : -1,\n",
    "                    }\n",
    "                }\n",
    "            ]))\n",
    "\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        runtimes.append(end-start)\n",
    "    \n",
    "    mean_runtimes.append(np.mean(runtimes))\n",
    "    std_runtimes.append(np.std(runtimes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir erhalten mit unserer Abfrage, eine Liste mit den durchschnittlichen Temperaturen pro Minute für für den Sensor: `Lufttemperatur 2-BME680`. Die Daten sind in einer Liste aus Dictionaries abgespeichert. Für eine bessere Darstellung und einfachere VIsualisierung wandeln wir die Daten noch in ein Dataframe um. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "df_mongodb_query = pd.DataFrame(query)\n",
    "df_mongodb_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup InfluxDB \n",
    "\n",
    "Das Setup der Influx-Datenbank ist gut Dokumentiert. Möchten Sie dieses Notebook ausführen müssen sie die Datenbank installieren. Besuchen Sie dazu die Website [influxdata.com](influxdata.com) und laden Sie das auf ihr Betriebsystem passende Programm herunter. Folgen Sie für das initiale Setup den Anweisungen der Website und des Intstallationsprogramm. Um dieses Notebook auszuführen muss die Organisation \"ddi\" genannt werden. Weiter ist die erstellung eines sogenannten \"Buckets\" notwendig. Dieser kann im Influx-Dashboad unter \"data -> bucket\" erstellt werden. Der Bucket muss den Namen \"ddi\" tragen. Die Retention Policy kann auf \"Never\" gesetzt werden. Für die Kommunikation zur Datenbank über Python muss die Bibliothek `influxdb_client` installiert und importiert werden. Dies Bibiliothek dient als Client und ist in etwa mit dem Cursor für eine Postgres-Datenbank zu vergleichen. Beachten Sie dass der Zugriff authorisiert sein muss. Damit eine Verbindung zur Datenbank erfolgreich ist muss eine Identifikation mit einem persönlichen Token erfolgen. Dieser Token ist ebenfalls auf dem Influx-Datenbank Dashboard erhältlich unter \"data -> token\". Ersetzen sie den Token (Variable `token`) in der unteren Code-Zelle mit ihrem persönlichen Token   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install influxdb-client\n",
    "from datetime import datetime\n",
    "\n",
    "from influxdb_client import InfluxDBClient, Point, WritePrecision\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "\n",
    "# You can generate a Token from the \"Tokens Tab\" in the UI\n",
    "token = \"uZei_UmVg7IGcllVQdvKbmbCjwx5s0pe7KfTafVspsL0qGWIg6fmB34JNwWmsEGdt9aFr2Qio6ltOB9_ZrCDDw==\"\n",
    "org = \"ddi\"\n",
    "bucket = \"ddi\"\n",
    "\n",
    "client = InfluxDBClient(url=\"http://localhost:8086\", token=token)\n",
    "write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "\n",
    "# RS: uZei_UmVg7IGcllVQdvKbmbCjwx5s0pe7KfTafVspsL0qGWIg6fmB34JNwWmsEGdt9aFr2Qio6ltOB9_ZrCDDw==\n",
    "# SL: AW-zLqzOTpQW4sRYaKbdXpSxBLkxT8rT-RZA-IS5MYo41RZ40YoOCoNYTyu9S2La5W4KpcDzDgCfj53fk6aZuw=="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ist die Verbindung erfolgreich, so gibt die untere Code-Zelle als Status \"pass\" an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.health()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query MongoDB, insert into InfluxDB\n",
    "\n",
    "Wir transferieren die Daten die wir für die unsere Analyse verwenden möchten in die InfluxDB indem wir die Daten für den Passenden Sensor in den Bucket \"ddi\" übertragen. Dazu führen wir ein Query aus welches die Daten des Sensors `IFU Sensebox20121 2A` abfragt. Dise Daten transferieren wir dann mit der `write_api()` von InfluxDB zu unserem \"ddi\"-Bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"IFU Sensebox2021 2A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_names = mongo_db[col].find().distinct('sensor_name')\n",
    "measurment_id = mongo_db[col].find().distinct('_id')\n",
    "results = []\n",
    "ts, vs = {}, {}\n",
    "for j in tqdm(measurment_id):\n",
    "    sample = mongo_db[col].find_one({'_id':j},{'value', 'timestamp', 'sensor_name', \"box_name\"})\n",
    "    indlux_id = \"{}_{}\".format(sample[\"box_name\"], sample[\"sensor_name\"])\n",
    "    if indlux_id not in ts.keys():\n",
    "        ts[indlux_id] = []\n",
    "        vs[indlux_id] = []\n",
    "    ts[indlux_id].append(sample[\"timestamp\"].strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    vs[indlux_id].append(sample[\"value\"])\n",
    "#       print(ts, vs)\n",
    "# results[i] = dict(zip(ts, vs))\n",
    "\n",
    "\n",
    "for key in ts.keys():\n",
    "    result = dict()\n",
    "    result[\"_id\"] = key\n",
    "    result[\"measurments\"] = dict(zip(ts[key], vs[key]))\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in tqdm(results):\n",
    "    for observation in result['measurments'].items():\n",
    "        point = Point(result['_id'].split('_')[0]) \\\n",
    "          .tag(\"sensor_name\", result['_id'].split('_')[1]) \\\n",
    "          .field(\"_value\", observation[1])\\\n",
    "          .time(datetime.strptime(observation[0], \"%Y-%m-%d %H:%M:%S\"), WritePrecision.S)\n",
    "\n",
    "        write_api.write(bucket, org, point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query InfluxDB\n",
    "\n",
    "Wir führen nun die selbe Afrage aus wie bei der MongoDB zuvor. Dabei nehmen wir den minütlichen Durchschnitt des Sensors \"Lufttemperatur 2-BME680\" der Box \"IFU Sensebox2021 2A\". Wir steigern mit jeder Iteration den Messbereich um vier Stunden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_runtimes_i = []\n",
    "std_runtimes_i = []\n",
    "n_init = 5\n",
    "\n",
    "api = client.query_api()\n",
    "\n",
    "for i in tqdm(range(len(times))):   \n",
    "    runtimes = [] \n",
    "    n_datepoints = []\n",
    "    \n",
    "    for _ in range(n_init):\n",
    "\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        query = f'''\n",
    "            from(bucket: \"{bucket}\")\n",
    "              |> range(start: time(v: \"{from_date}\"), stop: time(v: \"{times[i]}\"))\n",
    "              |> filter(fn: (r) => r[\"_measurement\"] == \"IFU Sensebox2021 2A\")\n",
    "              |> filter(fn: (r) => r[\"_field\"] == \"_value\")\n",
    "              |> filter(fn: (r) => r[\"sensor_name\"] == \"Lufttemperatur 2-BME680\")\n",
    "              |> aggregateWindow(every: 1m, fn: mean, createEmpty: false)\n",
    "              |> yield(name: \"mean\")'''\n",
    "\n",
    "        table = api.query_data_frame(query, org=org)\n",
    "\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        runtimes.append(end-start)\n",
    "        n_datepoints.append(len(table))\n",
    "    \n",
    "    mean_runtimes_i.append(np.mean(runtimes))\n",
    "    std_runtimes_i.append(np.std(runtimes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leistungsvergleich\n",
    "\n",
    "Die folgende Grafik zeigt die Laufzeit bei steigender Anzahl Datenpunkte für eine Aggregation (Mittelwert für jede Minute) in der MongoDB sowie der InfluxDB. Wir erkennen, das die InfluxDB konstant schneller ist als die MongoDB und auch weniger variiert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = '%Y-%m-%dT%H:%M:%S.%fZ'\n",
    "time_diff = [(datetime.strptime(i, fmt)-datetime.strptime(from_date, fmt)) for i in times]\n",
    "time_diff = [time.total_seconds()/60 for time in time_diff]\n",
    "\n",
    "plt.plot(time_diff, mean_runtimes_i, color='green', label='InfluxDB')\n",
    "plt.fill_between(time_diff, np.subtract(mean_runtimes_i,std_runtimes_i), np.add(mean_runtimes_i,std_runtimes_i), alpha=.2, color='green')\n",
    "plt.plot(time_diff, mean_runtimes, color='orange', label='MongoDB')\n",
    "plt.fill_between(time_diff, np.subtract(mean_runtimes,std_runtimes), np.add(mean_runtimes,std_runtimes), alpha=.2, color='orange')\n",
    "plt.legend()\n",
    "plt.xlabel(\"queried timerange [min]\")\n",
    "plt.ylabel(\"query duration [s]\")\n",
    "plt.title('Laufzeit bei steigender Anzahl Datenpunkte')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ab wann lohnt es sich die MongoDB als Data Lake zu verwenden und die InfluxDB zur Analyse hinzuzuziehen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch wenn wir es in unserem Beispiel nicht reproduzieren konnten, ist InfluxDB besser für TimeSeries Analysen geeignet als die MongoDB (vgl. [1]). Der Zeitpunkt ab dem es sich lohnt die MongoDB in Kombination mit der InfluxDB zu verwenden ist von der Anzahl Datenpunkte, der Form der Analyse und der verfügbaren Hardware abhängig. \n",
    "Um eine fundierte Entscheidung zu treffen, müssen folgende Paramter bekannt sein: \n",
    "\n",
    "- $n$ : Frei gewählte, minimale Anzahl Datenpunkte \n",
    "- $x$ : Anzahl Datenpunkte\n",
    "- $\\alpha$ : Dauer zur Ausführung eines Queries in der MongoDB für $n$ Datenpunkt\n",
    "- $\\beta$ : Dauer zur Ausführung eines Queries in der InfluxDB für $n$ Datenpunkt\n",
    "- $\\gamma$: Dauer des Datentransfers von der MongoDB zur InfluxDB bei $x$ Datenpunkten\n",
    "- $a$ : Minimale Laufzeit eines Queries in der MongoDB\n",
    "- $b$ : Minimale Laufzeit eines Queries in der InfluxDB\n",
    "\n",
    "\n",
    "Formel zur Berechnung der Laufzeit in der MongoDB, wobei $y$ die Laufzeit darstellt:\n",
    "\n",
    "$$\n",
    "a + \\alpha * x = y \n",
    "$$\n",
    "\n",
    "Formel zur Berechnung der Laufzeit in der InfluxDB:\n",
    "\n",
    "$$\n",
    "b + \\beta * x = y\n",
    "$$\n",
    "\n",
    "Basierend auf diesen beiden Formeln, können wir eine Ungleichung aufstellen die uns zeigen kann, ab wann es sich lohnt beide Datenbanken zu verwenden. Da bei der Verwendung von zwei Datenbanken zusätzlich zum Query auch noch ein Datenfransfer stattfinden muss, beziehen wir die Variable $\\gamma$ mit ein. Die Formel lautet somit: \n",
    "\n",
    "$$\n",
    "a + \\alpha * x > \\gamma + b + \\beta * x\n",
    "$$\n",
    "\n",
    "Wobei die linke Seite die Laufzeit in der MongoDb angiebt und die rechte Seite die Laufzeit in der InfluxDB zusätzlich zur Zeit die der Datentransfer aus der MongoDB mit einnimmt. Ist nun die linke Seite grösser als die rechte Seite, lohnt sich die Verwendung der MongoDB als Data Lake in Kombination mit der InfluxDB als Analysetool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erkenntnisse\n",
    "Wir haben in diesem Notebook die Verwendung einer MongoDB als Data Lake in Kombination zu einer InfluxDB für Analysezwecke getestet. Dabei haben wir eine MongoDB aufgesetzt welche Sensordaten (Time Series Data) von der [opensensemap](opensensemap.org)-API erhält. Wir testen die Aggregation der Messwerte eines spezifischen Sensors über einen bestimmten Zeitraum auf der MongoDB, sowie auf der InfluxDB. Wir erkennen an der Gegenüberstellung der Resultate, das zumindest für eine Datenmenge von ca. 170'000 Messwerten die Influxdatenbank schneller ist als die MongoDB. Bei einer solchen, geringen Anzahl an Datenpunkten, hängen die Resultate stark von der momentan Auslastung des Systems ab und sind somit nicht robust. \n",
    "\n",
    "Wir konnten mit dieser Anzahl Datenpunkte keinen definitiven Zeitunpkt angeben, ab dem sich ein wechsel zum dualen System rechtfertigen liesse. Im vorhergehenden Abschnitt haben wir allerdings eine Formel eingeführt die Abhängig von der verwendeten Hardware und der Anzahl Datenpunkte angiebt, ob sich die Verwendung einer MongoDB in Kombination mit der InfluxDB tatsächlich lohnt.\n",
    "\n",
    "Wir kommen zum Schluss, dass die vorgeschlagene Verwendung von MongoDB + InfluxDB erst ab einer grossen Datenmenge relevant scheint, sowie stark von folgenden Faktoren abhängt:\n",
    "\n",
    "- Anzahl Datenpunkte\n",
    "- Art der Aggregation (Moving Average, Mean etc.)\n",
    "- Verfügbare Hardware \n",
    "- Netzwerkgeschwindigkeit und Banndbreite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quellen\n",
    "\n",
    "[1]«MongoDB vs InfluxDB | InfluxData Time Series Workloads», InfluxData, Dez. 18, 2018. https://www.influxdata.com/blog/influxdb-is-27x-faster-vs-mongodb-for-time-series-workloads/ (zugegriffen Juni 17, 2021).\n",
    "\n",
    "[2]«MongoDB disk and memory requirements», Documentation & User Guides | FotoWare, Nov. 17, 2015. https://learn.fotoware.com/On-Premises/FotoWeb/05_Configuring_sites/Setting_the_MongoDB_instance_that_FotoWeb_uses/MongoDB_disk_and_memory_requirements (zugegriffen Juni 17, 2021).\n",
    "\n",
    "[3]H. 16 A. 2018 at 12:26, «InfluxDB design guidelines to avoid performance issues», Service  Engineering (ICCLab & SPLab). https://blog.zhaw.ch/icclab/influxdb-design-guidelines-to-avoid-performance-issues/ (zugegriffen Juni 17, 2021).\n",
    "\n",
    "[4]P. Dix, «InfluxDB Clustering - High Availability and Scalability», InfluxData, Sep. 10, 2020. https://www.influxdata.com/blog/influxdb-clustering/ (zugegriffen Juni 17, 2021).\n",
    "\n",
    "[5]«Security — MongoDB Manual», https://github.com/mongodb/docs-bi-connector/blob/DOCSP-3279/source/index.txt. https://docs.mongodb.com/manual/security/ (zugegriffen Juni 21, 2021).\n",
    "\n",
    "[6]«Manage InfluxDB security | InfluxDB OSS 1.8 Documentation». https://docs.influxdata.com/influxdb/v1.8/administration/security/ (zugegriffen Juni 21, 2021).\n",
    "\n",
    "[7]«Why Use MongoDB & When to Use It?», MongoDB. https://www.mongodb.com/why-use-mongodb (zugegriffen Juni 21, 2021).\n",
    "\n",
    "[8]«How to Scale MongoDB», MongoDB. https://www.mongodb.com/basics/scaling (zugegriffen Juni 21, 2021).\n",
    "\n",
    "[9] «Time Series Data and MongoDB: Part 2 – Schema Design Best Practices», MongoDB, Sep. 12. 2019, https://www.mongodb.com/blog/post/time-series-data-and-mongodb-part-2-schema-design-best-practices\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
