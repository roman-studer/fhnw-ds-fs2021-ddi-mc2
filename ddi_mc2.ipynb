{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import warnings\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "from datetime import datetime\n",
    "from isodate import datetime_isoformat\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ddi - Mini-Challenge zu LE3, NoSQL\n",
    "Roman Studer, Simon Luder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case\n",
    "<p align=\"center\">\n",
    "  <img src=\"./data/images/Konzept.png\" alt=\"drawing\" width=\"900\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir unterbreiten den Vorschlag eine NoSQL Datenbank (MongoDB) als Data Lake für die Speicherung von Time Series Daten zu verwenden und die Analyse besagter Daten auf einer Time-Series Datenbank durchzuführen. Die Analyse dieser Daten kann mittels einer auf Time Series optimierte Datenbank auf Abruf geschehen. Als Beispiel verwenden wir als Datenquelle openSenseMap, welche Messwerte und Sensormetainformationen über eine API zur Verfügung stellt. Ein aktiver Sensor sendet periodisch (je nach Sensor alle paar Sekunden oder Minuten) einen Messwert. Bei einem Intervall von 10 Sekunden sendet ein Sensor pro Jahr 3'153'600 Datenpunkte. \n",
    "\n",
    "OpenSenseMap erlaubt es ein ganzes Gebiet (Mittels Angabe von Breiten- und Längengrad) zu überwachen. Die Anzahl Sensoren, sowie deren Attribute kann sich über die Zeit ändern. Wenn zum Beispiel ein neuer Sensor im gleichen Gebiet in Betrieb genommen wird. Daher ist die Datenspeicherung in einer NoSQL, schemenlosen Datenbank geeignet. MongoDB ist dabei aufgrund des flexigblem Schema und einfacher horizontaler Skalierung gut geeignet. Dadurch sind wir auf Änderungen in den durch die API erhaltenen Attributen, sowie auf grosse Änderungen in der Datenmenge gewappnet. MongoDB ist allerdins nicht für die Analyse von Time Series geeignet. InfluxDB, eine Time Series Datenbank, ist für Time Series Daten optimiert und kann schnell Aggregationen über eine grosse Anzahl von Datenpunkten (über Timestamp Indexiert) durchführen. Ein Beispiel wäre ein Moving Average mit kleinem Fenster über mehrere Millionen Datenpunkte.\n",
    "\n",
    "Weiter existiert ein MongoDB-Plugin auf Telegraf welche die Performance der MongoDB überwachen kann. Somit kann das Monitoring über Influx betrieben werden. \n",
    "\n",
    "Im Anschluss setzen wir sowohl eine MongoDB als auch eine InfluxDB auf. Über ein Script laden wir alle Sensordaten auf dem Gelände der ETH-Zürich, welche OpenSenseMap zur Verfügung stellt herunter und speichern diese in der MongoDB. Im Anschluss messen wir die Zeit für Aggregationen bei steigenden Datenpunkten einzeln für beide Datenbanken, sowie für den Fall wenn MongoDB als Datalake verwendet wird. Dadurch können wir einen Punkt identifizieren ab dem es nicht mehr sinnvoll ist nur mit einer MongoDB zu arbeiten, sondern die InfluxDB zur Analyse hinzuzuziehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergleich MongoDB vs. InfluxDB\n",
    "\n",
    "Einen Einblick in die beiden Datenbanken zu erhalten, erlaubt dass Dokument [1] \"Benchmarking InfluxDB vs. MonoDB for Time Series Data, Metrics % Management\" (siehe Anhang),\n",
    "\n",
    "| Bereich  | InfluxDB  |MongoDB  | \n",
    "|---|---|---|\n",
    "| **Verwendung** | Time-Series Datenbank | Dokumentenbasierte Datenbank, NoSQL Datenbank | \n",
    "| **Memory-Auslastung**  | ca 2-4 GB pro <100'000 Einträge [2]  | ca. 1 GB pro 100'000 Einträge [3] |\n",
    "| **Festplattenauslastung über 24h** [1]  | 178 MB |  34820 MB  |\n",
    "| **Abfragegeschwindigkeit** (Queries per Second, 1000 Einträge) [1] |  935 | 164  |\n",
    "| **Einfügegeschwindigkeit** (Werte pro Sekunde) [1] | 2,800,990  | 1,114,616  |\n",
    "| **Skalierbarkeit**  | Horizontal Skalierbar (Clustering) bei InfluxDB Enterprise [4] | Horizontal Skalierbar  |\n",
    "| **Sicherheit**  | Authentifizierung  | Authentifizierung/Verschlüsselung  |\n",
    "| **Sprache**[1]  | C/C++  | Go |\n",
    "| **Schema**[1]  | Schemaless   | Schemaless |\n",
    "\n",
    "**Sicherheit:** MongoDB bietet diverse Features die die Sicherheit der Datenbank erhöhen können. Die Datenbank unterstützt under anderem mehrere Authentifizierungsmechanismen wie `SCRAM` (\"Salted Challenge Response Authentication Mechanism\", mit SHA-1 oder SHA-256), womit Name, Passwort, Authentifizierungsdatenbank geprüft werden, aber auch das `x-509 Certificate`, die `LDAP`- und `Kerberos`-Authentifizierung. Zugriff kann durch eine rollenbasierte Zugriffskontrolle reguliert werden. So können Benutzer erstellt werden, welchen ähnlich zu einem Active Directory Rollen (wie zum Beispiel \"Administrator\") vergeben werden können, welche ihren Handlungsbereich einschränkt. Ein weiteres Feature ist die Client-Side Field Level Encryption. So können Felder eines Dokumentes vor der Übermittlung zum Server bereits verschlüsselt werden.(vgl. [5]) Die InfluxDB bietet ebenfalls Authentifizierungsmethoden an (darunter Authentifizierung über CLI, JWT Token oder über die API). Dabei wird ebenfalls ein Rollensystem verwendet welches die Verwaltung von Benutzern und derer Berechtigungen ermöglicht. Die Kommunikation zwischen Client und Server läuft über HTTPS, wobei die Authentizität des Servers geprüft wird. (vgl. [6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenmodell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./data/images/ER-Diagram.png\" alt=\"drawing\" width=\"150\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of ifu boxes at eth\n",
    "url = 'https://api.opensensemap.org/boxes?'\n",
    "bbox = '8.50269672304309, 47.40598032642525,  8.512126181507432, 47.4113301084323 ' # boundary box around eth zurich\n",
    "boxes = requests.get(url, params={'bbox':bbox, 'full':'false'}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date = '2021-04-25T10:05:49.581Z'\n",
    "to_date = '2021-05-02T10:05:49.581Z'\n",
    "data_format = 'csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = pd.date_range(from_date,to_date,freq='4h')\n",
    "\n",
    "def to_RFC3339Date(x):\n",
    "    x = str(x)\n",
    "    x = x.replace(' ','T')\n",
    "    x = x.replace('000+00:00','Z')\n",
    "    return x\n",
    "\n",
    "times = [i for i in times][1:] # remove first time as it is equal to from_date\n",
    "times = [to_RFC3339Date(i) for i in times]\n",
    "len(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for box in tqdm(boxes):\n",
    "    box_id = box['_id']\n",
    "    box_name = box['name']\n",
    "    location = box['currentLocation']['coordinates']\n",
    "    lat, lon = location[0], location[1]\n",
    "    for sensor in box['sensors']:\n",
    "        try:\n",
    "            sensor_id = sensor['_id']\n",
    "            sensor_name = sensor['title']\n",
    "            sensor_name.replace('/', '')\n",
    "            sensor_unit = sensor['unit']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #url = f'https://api.opensensemap.org/boxes/{box_id}/data/{sensor_id}?format={data_format}&download=true'\n",
    "        url = f'https://api.opensensemap.org/boxes/{box_id}/data/{sensor_id}?from-date={from_date}&to-date={to_date}&download=true&format={data_format}'\n",
    "        r = requests.get(url, stream=True)\n",
    "        if (len(r.text) > 16): # check if sensor returns values (header has length 16)\n",
    "            with open(f'./data/{box_name}_{sensor_name}.csv', 'wb') as f:\n",
    "                for _, line in enumerate(r.iter_lines()):\n",
    "                    if _ == 0: # define header\n",
    "                        line = 'box_name,sensor_name,box_id,sensor_id,lat,lon,unit,current_time,value\\n'\n",
    "                    else:\n",
    "                        timest, value = (line.decode(\"utf-8\").split(','))\n",
    "                        timest = timest.replace('T', ' ').replace('Z', '')\n",
    "                        timest = datetime.strptime(timest, '%Y-%m-%d %H:%M:%S.%f').strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        line= f'{box_name},{sensor_name},{box_id},{sensor_id},{lat},{lon},{sensor_unit},{timest},{value}\\n'\n",
    "                    f.write(line.encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymongo\n",
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_dict(path, file):\n",
    "    '''converts a csv file to a list of dictionaries'''\n",
    "    data = pd.read_csv(path + file)\n",
    "    pd.to_datetime(data.current_time, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    document = []\n",
    "    for i in range(len(data)):\n",
    "        dictionary = dict()\n",
    "        dictionary[\"_id\"] = file.replace('.csv', '_{}'.format(i))\n",
    "        dictionary[\"box_name\"] = data[\"box_name\"][0]\n",
    "        dictionary[\"sensor_name\"] = data[\"sensor_name\"][0]\n",
    "        dictionary[\"box_id\"] = data[\"box_id\"][0]\n",
    "        dictionary[\"sensor_id\"] = data[\"sensor_id\"][0]\n",
    "        dictionary[\"lat\"] = data[\"lat\"][0]\n",
    "        dictionary[\"lon\"] = data[\"lon\"][0]\n",
    "        dictionary[\"unit\"] = data[\"unit\"][0]\n",
    "        dictionary[\"value\"] = data[\"value\"][i]\n",
    "        dictionary[\"timestamp\"] = dt.datetime.strptime(data[\"current_time\"][i], '%Y-%m-%d %H:%M:%S')\n",
    "        document.append(dictionary)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = pymongo.MongoClient('localhost', 27017)\n",
    "mongo_db = mongo_client[\"ddi_mc2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate db\n",
    "\n",
    "In der nächsten Zelle werden die Sensoren in die MongoDB geladen. Als erster Schritt werden alle Dokumente mit der Endung \".csv\" gesucht. Ein einzelnes Dokument enthält jeweils alle Informationen inklusive der Messwerte für einen einzelnen Sensor. Danach wird jedes Dokument, mit der funktion `csv_to_dict()`, pro Zeile getrennt in ein Dictionary umgewandelt. Die Dictionaries werden dann einzeln mit dem Befehl `insert_one()` in die MongoDB geladen. \n",
    "\n",
    "Die Messwerte der einzelnen Sensoren sind in der MongoDB jeweils nach ihrer Zugehörigkeit zu einer Messbox, in einer Collection zusammengefasst. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "path = './data/'\n",
    "for file in tqdm(os.listdir(\"./data\")):\n",
    "#     print(file.split(\"_\")[0])\n",
    "    if file.endswith('.csv'): # check for filetype\n",
    "        if not mongo_db[file.split(\"_\")[0]].count_documents({\"_id\":file.replace('.csv', '')}) > 0:\n",
    "            dictionary = csv_to_dict(path, file)\n",
    "            for document in dictionary:\n",
    "                mongo_db[file.split(\"_\")[0]].insert_one(document)\n",
    "            if VERBOSE:\n",
    "                print(\"populate:\", file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Sensor-Boxes within the MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mongo_db[\"IfU SenseBox2021 11A\"].find().distinct('_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup InfluxDB \n",
    "\n",
    "Das Setup der Influx-Datenbank ist gut Dokumentiert. Möchten Sie dieses Notebook ausführen müssen sie die Datenbank installieren. Besuchen Sie dazu die Website [influxdata.com](influxdata.com) und laden Sie das auf ihr Betriebsystem passende Programm herunter. Folgen Sie für das initiale Setup den Anweisungen der Website und des Intstallationsprogramm. Um dieses Notebook auszuführen muss die Organisation \"ddi\" genannt werden. Weiter ist die erstellung eines sogenannten \"Buckets\" notwendig. Dieser kann im Influx-Dashboad unter \"data -> bucket\" erstellt werden. Der Bucket muss den Namen \"ddi\" tragen. Die Retention Policy kann auf \"Never\" gesetzt werden. Damit eine Verbindung zur Datenbank erfolgreich ist muss eine Identifikation mit einem persönlichen Token erfolgen. Dieser Token ist ebenfalls auf dem Influx-Datenbank Dashboard erhältlich unter \"data -> token\". Ersetzen sie den Token (Variable `token`) in der unteren Code-Zelle mit ihrem persönlichen Token   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = mongo_db['IFU Sensebox2021 2A'].find().distinct('sensor_name')[0:10]\n",
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_name = \"Lufttemperatur 2-BME680\"\n",
    "box_name = \"IFU Sensebox2021 2A\"\n",
    "\n",
    "mean_runtimes = []\n",
    "std_runtimes = []\n",
    "n_init = 10\n",
    "\n",
    "for i in tqdm(range(len(times))):   \n",
    "    runtimes = [] \n",
    "    n_datepoints = []\n",
    "    \n",
    "    for _ in range(n_init):\n",
    "\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        query = list(mongo_db[box_name].aggregate([\n",
    "                {\n",
    "                    \"$match\": {\n",
    "                        \"sensor_name\": sensor_name,\n",
    "                        \"timestamp\": {\"$gte\": datetime.strptime(from_date, '%Y-%m-%dT%H:%M:%S.%fZ'), \n",
    "                                      \"$lte\": datetime.strptime(times[i], '%Y-%m-%dT%H:%M:%S.%fZ')}\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"$group\": {\n",
    "                        \"_id\": {\n",
    "                        \"minute\": {\"$minute\": \"$timestamp\"},\n",
    "                        \"hour\": {\"$hour\": \"$timestamp\"},\n",
    "                        \"day\": {\"$dayOfMonth\": \"$timestamp\"},\n",
    "                        \"month\": {\"$month\": \"$timestamp\"},\n",
    "                        \"year\": {\"$year\": \"$timestamp\"} \n",
    "                        }, \n",
    "                        \"mean\": {\"$avg\": \"$value\"},\n",
    "                        \"unit\":{\"$first\": \"$unit\"},\n",
    "                        \"timestamp\": { \"$max\": \"$timestamp\" }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"$project\":{\n",
    "                        \"_id\":0,\n",
    "                        \"mean\": \"$mean\",\n",
    "                        \"unit\": \"$unit\",\n",
    "                        \"timestamp_minute\": {\"$dateToString\":{\"format\": \"%Y-%m-%d %H:%M:00\", \n",
    "                                                              \"date\": \"$timestamp\" }}\n",
    "                    }\n",
    "                },\n",
    "                { \n",
    "                    \"$sort\": {\n",
    "                        \"timestamp_minute\" : -1,\n",
    "                    }\n",
    "                }\n",
    "            ]))\n",
    "\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        runtimes.append(end-start)\n",
    "    \n",
    "    mean_runtimes.append(np.mean(runtimes))\n",
    "    std_runtimes.append(np.std(runtimes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ist die Verbindung erfolgreich, so gibt die untere Code-Zelle als Status \"pass\" an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checks': [],\n",
       " 'commit': '4db98b4c9a',\n",
       " 'message': 'ready for queries and writes',\n",
       " 'name': 'influxdb',\n",
       " 'status': 'pass',\n",
       " 'version': '2.0.6'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "df_mongodb_query = pd.DataFrame(query)\n",
    "df_mongodb_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mongodb_query[\"timestamp_minute\"] = pd.to_datetime(df_mongodb_query.timestamp_minute)\n",
    "plt.plot(df_mongodb_query[\"timestamp_minute\"], df_mongodb_query[\"mean\"], \".\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = '%Y-%m-%dT%H:%M:%S.%fZ'\n",
    "time_diff = [(datetime.strptime(i, fmt)-datetime.strptime(from_date, fmt)) for i in times]\n",
    "time_diff = [time.total_seconds()/60 for time in time_diff]\n",
    "\n",
    "plt.plot(time_diff, mean_runtimes)\n",
    "plt.fill_between(time_diff, np.subtract(mean_runtimes,std_runtimes), np.add(mean_runtimes,std_runtimes), alpha=.2)\n",
    "plt.xlabel(\"queried timerange [min]\")\n",
    "plt.ylabel(\"query duration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup InfluxDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install influxdb-client\n",
    "from datetime import datetime\n",
    "\n",
    "from influxdb_client import InfluxDBClient, Point, WritePrecision\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "\n",
    "# You can generate a Token from the \"Tokens Tab\" in the UI\n",
    "token = \"AW-zLqzOTpQW4sRYaKbdXpSxBLkxT8rT-RZA-IS5MYo41RZ40YoOCoNYTyu9S2La5W4KpcDzDgCfj53fk6aZuw==\"\n",
    "org = \"ddi\"\n",
    "bucket = \"ddi\"\n",
    "\n",
    "client = InfluxDBClient(url=\"http://localhost:8086\", token=token)\n",
    "write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "\n",
    "# RS: uZei_UmVg7IGcllVQdvKbmbCjwx5s0pe7KfTafVspsL0qGWIg6fmB34JNwWmsEGdt9aFr2Qio6ltOB9_ZrCDDw==\n",
    "# SL: AW-zLqzOTpQW4sRYaKbdXpSxBLkxT8rT-RZA-IS5MYo41RZ40YoOCoNYTyu9S2La5W4KpcDzDgCfj53fk6aZuw=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.health()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query MongoDB, insert into InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"IFU Sensebox2021 2A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get all sensors within a selected box**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_names = mongo_db[col].find().distinct('sensor_name')\n",
    "measurment_id = mongo_db[col].find().distinct('_id')\n",
    "results = []\n",
    "ts, vs = {}, {}\n",
    "for j in tqdm(measurment_id):\n",
    "    sample = mongo_db[col].find_one({'_id':j},{'value', 'timestamp', 'sensor_name', \"box_name\"})\n",
    "    indlux_id = \"{}_{}\".format(sample[\"box_name\"], sample[\"sensor_name\"])\n",
    "    if indlux_id not in ts.keys():\n",
    "        ts[indlux_id] = []\n",
    "        vs[indlux_id] = []\n",
    "    ts[indlux_id].append(sample[\"timestamp\"].strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    vs[indlux_id].append(sample[\"value\"])\n",
    "#       print(ts, vs)\n",
    "# results[i] = dict(zip(ts, vs))\n",
    "\n",
    "\n",
    "for key in ts.keys():\n",
    "    result = dict()\n",
    "    result[\"_id\"] = key\n",
    "    result[\"measurments\"] = dict(zip(ts[key], vs[key]))\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get sensor names within the Box**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in tqdm(results):\n",
    "    for observation in result['measurments'].items():\n",
    "        point = Point(result['_id'].split('_')[0]) \\\n",
    "          .tag(\"sensor_name\", result['_id'].split('_')[1]) \\\n",
    "          .field(\"_value\", observation[1])\\\n",
    "          .time(datetime.strptime(observation[0], \"%Y-%m-%d %H:%M:%S\"), WritePrecision.S)\n",
    "\n",
    "        write_api.write(bucket, org, point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_runtimes = []\n",
    "std_runtimes = []\n",
    "n_init = 10\n",
    "\n",
    "api = client.query_api()\n",
    "\n",
    "for i in tqdm(range(len(times))):   \n",
    "    runtimes = [] \n",
    "    n_datepoints = []\n",
    "    \n",
    "    for _ in range(n_init):\n",
    "\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        query = f'''\n",
    "            from(bucket: \"{bucket}\")\n",
    "              |> range(start: time(v: \"{from_date}\"), stop: time(v: \"{times[i]}\"))\n",
    "              |> filter(fn: (r) => r[\"_measurement\"] == \"IFU Sensebox2021 2A\")\n",
    "              |> filter(fn: (r) => r[\"_field\"] == \"_value\")\n",
    "              |> filter(fn: (r) => r[\"sensor_name\"] == \"Lufttemperatur 2-BME680\")\n",
    "              |> aggregateWindow(every: 1m, fn: mean, createEmpty: false)\n",
    "              |> yield(name: \"mean\")'''\n",
    "\n",
    "        table = api.query_data_frame(query, org=org)\n",
    "\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        runtimes.append(end-start)\n",
    "        n_datepoints.append(len(table))\n",
    "    \n",
    "    mean_runtimes.append(np.mean(runtimes))\n",
    "    std_runtimes.append(np.std(runtimes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, len(mean_runtimes)), mean_runtimes)\n",
    "plt.fill_between(range(0, len(mean_runtimes)), np.subtract(mean_runtimes,std_runtimes), np.add(mean_runtimes,std_runtimes), alpha=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quellen\n",
    "\n",
    "[1]«MongoDB vs InfluxDB | InfluxData Time Series Workloads», InfluxData, Dez. 18, 2018. https://www.influxdata.com/blog/influxdb-is-27x-faster-vs-mongodb-for-time-series-workloads/ (zugegriffen Juni 17, 2021).\n",
    "\n",
    "[2]«MongoDB disk and memory requirements», Documentation & User Guides | FotoWare, Nov. 17, 2015. https://learn.fotoware.com/On-Premises/FotoWeb/05_Configuring_sites/Setting_the_MongoDB_instance_that_FotoWeb_uses/MongoDB_disk_and_memory_requirements (zugegriffen Juni 17, 2021).\n",
    "\n",
    "[3]H. 16 A. 2018 at 12:26, «InfluxDB design guidelines to avoid performance issues», Service  Engineering (ICCLab & SPLab). https://blog.zhaw.ch/icclab/influxdb-design-guidelines-to-avoid-performance-issues/ (zugegriffen Juni 17, 2021).\n",
    "\n",
    "[4]P. Dix, «InfluxDB Clustering - High Availability and Scalability», InfluxData, Sep. 10, 2020. https://www.influxdata.com/blog/influxdb-clustering/ (zugegriffen Juni 17, 2021).\n",
    "\n",
    "[5]«Security — MongoDB Manual», https://github.com/mongodb/docs-bi-connector/blob/DOCSP-3279/source/index.txt. https://docs.mongodb.com/manual/security/ (zugegriffen Juni 21, 2021).\n",
    "\n",
    "[6]«Manage InfluxDB security | InfluxDB OSS 1.8 Documentation». https://docs.influxdata.com/influxdb/v1.8/administration/security/ (zugegriffen Juni 21, 2021).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
